---
layout: cv
title: MINYU Gao's CV
pdf: true
---
# __高旻昱__

<div id="webaddress">
<i class="fi-telephone" style="margin-left:1em"></i>
<a style="margin-left:0.5em">+86-13661461891</a>
<i class="fi-mail" style="margin-left:1em"></i>
<a href="minyu.gao@outlook.com" style="margin-left:0.5em">minyu.gao@outlook.com</a>
<i class="fi-male-female" style="margin-left:1em"></i>
<a style="margin-left:0.5em">男</a><br>
<i class="fi-home" style="margin-left:1em"></i>
<a href="https://josepgao.github.io/ChatJSP/" title="chatJSP">访问我的个人chatgpt ➡️ josepgao.github.io/ChatJSP/</a>
</div>



## 教育经历

### __瑞士联邦理工（洛桑）EPFL__ `2013.9 - 2016.4`
```
瑞士，洛桑
```
- 计算机科学和生物医学工程学 Master of Science
- 深度学习，计算机图像视觉，医疗影像学，数字信号处理，机器学习
### __上海交通大学__ `2009.9 - 2013.6`
```
中国，上海
```
- 机械工程 Bachelor of Science
- 生物力学，优化理论，计算机仿真

## 主要能力概述
__计算机视觉__ 
	|目标检测 | <a href="#全景分割">全景分割</a> | <a href="#zero-shot">zero-shot</a>图像任务 | cross-domain迁移学习 |

__多模态__ 
	|图文、视频检索 Retrieval | 文本图像生成 | 通用多模态方案,如<a href="#CLIP">CLIP</a>, <a href="#ALBEF">ALBEF</a>, <a href="#BEiTV3">BEiTV3</a> |

__LLM__
	|Bert | GPT | promting engineering | scaling law |

__AI工程化__
	|数据工程 | 特征工程 | 多框架 | 边缘计算部署 | 大规模线上部署 | 模型并行化 | 模型轻量化 |

__管理经验__
	|AI算法团队及配套全栈软件团队(前后端)管理经验(4年) | 从概念到研发再到产品的全工作链流程(8年)| 与高校学术团队紧密合作经验(4年)| 

__Coding Skill__
|Python | Torch | Tensorflow | C/C++ | Linux | Docker | Kubernetes | 


## 工作经历简述

### __上海领健信息科技有限公司__ [上海] `2022.06 - now`
_算法负责人_<br>
* 负责算法团队的技术选型和核心技术研发
* 负责算法团队相关项管理

### __上海翎腾智能科技有限公司__ [上海] `2019.4 - 2022.06`
_人工智能算法负责人 & 技术合伙人_<br>

* 负责深度图像识别系统内多种功能模块的设计开发
* 负责技术团队相关项目管理

### __上海第五区科技有限公司__ [上海] `2016.5 - 2019.4`
_算法负责人_<br>

* 负责多项可穿戴嵌入式设备算法设计和研发
* 算法技术顾问（2017.12-2018.7）

### __CSEM,瑞士国家微电子研发中心__ [纳沙泰尔,瑞士] `2015.9 - 2016.4`
_算法研究员_<br>

* 非介入式血压机器学习方案,成功推动长期项目至最后阶段。（光生理信号模式识别和机器学习）
* Directed by Josep Solà I Càros

### __PSA,标志雪铁龙汽车全球研发总部__ [巴黎, 法国] `2015.3 - 2015.9`
_研发工程师_<br>

* 独立工作负责汽车人机交互项目的预研发（电生理信号模式识别）
* 建立一整套完整的从数字信号处理到模式识别的解决方案

<br>

## 领健工作内容细节 [ 2022.06 - now]

### __医患沟通用牙齿矫正模拟算法__
* 快速精确的牙齿矫正模拟功能,在<a href="#stablediffusion">stable diffusion</a>发布的最初阶段(2022年8月)即完成选型,定制化训练和本地工程化的工作.
* 针对diffusion族生成式网络进行深入研究,指导设计和架构适用于当前任务的新模型结构和约束，使得生成式模型有效性达到商用标准。

### __全颌可视化系统功能,算法侧架构__
* 基于锥形束CT数据(3D点云)的实例分割算法落地和研究优化.
* 多传感器数据高精度配准融合工作,数据源包括(CBCT,激光扫描,RGB图像,CT).通过掌握大量口腔解剖学知识,实现了超过业内最高水平的高精度的配准工作.
* 进展中发现了业内暂无人解决的基础性问题,协调组内的研究专家与业内专业教授联合研究。

### __算法在产品商业化上的应用实例__
* 配准用户多次不同时间段的牙齿三维扫描数据，分析形态进展与商业治疗计划的匹配性，解决商业化上的一个重要痛点（隐形矫正的重启率问题）
* 建立复杂的力学仿真方案，分析治疗过程中的颌骨和牙齿牙根结构之间的关系，用于产品的风险预警和管控。
* 基于大规模数据和硬性逻辑规则进行自动化的牙齿矫正算法架构设计，协调算法、医学、软件三个团队成功将新算法融合到现有成熟的商业化流程中，用于大幅提升人类设计师的工作效率，在服务于每天同样多的客服需求情况下，所需设计师总数量下降三倍。 

### __医疗影像识别任务综合平台__
* 架构一个全面的任务平台,用于多样的二维医疗影像数据的各类AI工作
* 实现多模态检索能力(使用文本搜索图像)
* 实现一般性的图片快速分类工作,落地成为内部服务,使得数据工程师可以从中快速自定义训练分类网络
* 设计和架构面向医疗影像的专业数据系统,包括医用的专业标注软件、数据检索系统，帮助在多个不同数据和任务上提供数据源
* 实现功能：Xray侧位片特征点定位，特殊组织分割，精确率达到行业内顶尖水平，对标行业内其他专业软件
* 实现功能：Xray全景片数据中，超过多达20种不同组织结构语义分割，超分辨率分割，细小目标结构定位，同类型组织的病理学阶段诊断，诊断可靠度（recall-94%），达到业内顶尖，超过很多已经商业化落地方案,与专业医生水平持平。
* 实现功能：支撑全部领健各类型数据的快速分类，部署，实现在毫秒级别的部署，支撑几千家用户的日常数据智能归档需求。

### __架构专业的医用计算机辅助诊断(CAD)平台__ 
* 与专业医学专家合作,消化行业内不同的需求列表.
* 综合需求列表,并根据需求解析成不同的算法工具链. 
* 特征点定位基础算法，用于在各类模态上进行关键的医用解剖学相关的特征点定位功能，用于为后续的诊断测量输出基础信息。
* RGB图像数据基础算法，包括人脸识别，口内照片的各类目标识别和实例分割
* Xray数据基础算法，包括侧位片，全景片，正位片的各类目标识别和实例分割
* 跨模态数据融合基础算法，能够有效融合上述三类数据，用于不同模态数据的协同分析工作，用于在不同的诊断场景下进行输出。

<div STYLE="page-break-after: always;"></div> 

## 翎腾工作内容细节 [ 2019.05 - 2022.05] 

### __机器人自主感知机械臂项目__ `2022 @翎腾`

* 机器人机械臂基于视觉反馈和力学反馈的自主抓握感知能力研究
  * 视觉数据的多模态理解,设计使用CLIP和ALBEF多模态框架完成视频流中的物体的检索
  * 检索来源于文字流,为未来直接接入语音控制做准备.打通语音,文字,图片,动作理解之间的跨模态通道
  * 使用mujoco框架来搭建其中的力学仿真环境,使用虚幻引擎搭建视觉仿真环境
  * 视觉(虚幻)和力学环境(mujoco)与通过同步通信,使得深度强化学习的agent可以同步在两个环境中获取反馈数据
  * 研究从虚拟环境到真实环境的迁移问题,并设计一套完整的迁移方案,使得agent可以在虚拟环境中学习到的知识可以在真实环境中得到应用

* 机器人3D视觉功能
  1. 熟悉和跟进机器人3D视觉研发前沿潮流（双目，单目，till 2022.05）
  2. 独立设计完善的基于虚幻引擎的机器人仿真视觉环境。并部署强化学习
  3. 完整架构和设计全新的视觉感知系统
  4. 设计全新架构基于transformer和CNN的神经网络结构,实现处理视频流理解和数据模态转换的神经网络
  5. 完成全部的数据链搭建，框架搭建，算法训练，算法量化，嵌入式终端部署工作，为机器人赋予视觉感知能力

### __基于计算机视觉的智能阅读学习设备__ `2019,2020,2021 @翎腾`

* 设计完整的计算机视觉和NLP算法框架,完成工作链中全部算法模型选型,架构设计,训练,轻量化,并部署到嵌入式设备中进行边缘运算
* 全场景OCR功能,支持超过20种印刷体文字,支持手写体英文和中文识别
* 手写体准确率水平对标腾讯优图，2020.10,印刷体准确率水平超过99%，对标业内绝大部分的OCR方案（百度，腾讯，科大讯飞,2021.03）
* 功能触发模块应用不同的触发物体识别,同时引入视频流理解算法架构,使得系统可以在保证对用户理解准确性的同时覆盖更多的任务场景,做到一种设备覆盖全部阅读场景.
* 主要的任务场景包括单词点读,交互式的多词或句子点读和翻译,手写过程跟踪,手写内容理解,多指手势等等.


### __个人核心研究方向__ `2019,2020,2021 @翎腾`

* zero-shot方案的CV任务, 如CLIP,Moco等对比学习或无监督学习方案.
* cross-domain gap,跟进数据增强,迁移学习,多任务学习,域自适应学习.
* 目标检测、全景分割、ReID等下游任务，具体网络包括DETR, SwinTransformer,MAE,yolo(v5,v3,yolox),centernet,Efficient-det,MaskRCNN等
* 基于时序的动态视频数据下的目标识别 
* 基于空间数据的3d数据的目标识别,ACVNet,PointNet++
* 多模态数据的理解和融合

### __算法在产品商业化上的应用实例__ `2019,2020,2021 @翎腾`

* 阅读学习场景的智能硬件（台灯，点读设备）
  1. 阅读辅助，单词点读识别功能
  2. 交互跟踪划线词句识别功能
  3. 书写过程笔迹跟踪和手写内容理解
  4. 双指交互自动摘抄功能
  5. 全线产品共三代硬件的迭代升级，总体出货量约10万+

* 教育辅助场景(教师用自动批改设备)
  1. 自动识别页面精确范围算法
  2. 自动识别页面信息算法（书本信息和页码）
  3. 针对常规练习册系统的自动判卷完整功能
  4. 针对常规练习册系统的精细化内容分离算法
  5. 支持了超过3万+教育系统使用者.日活用户超过1000+人.
  
### __深度学习数据系统__ `2019,2020,2021 @翎腾`

* 搭建完整的数据采集和实机测试系统
  1. 在嵌入式端使用模拟设备进行数据采集和离线数据管理
  2. 采集系统自动化采集、并进行记录和自动化上传
  3. 系统同时可以由采集人员完成实机设计场景测试

* 设计并指导开发数据标注系统
  1. 适配多功能的标注任务
  2. 完全自动化配置方案
  3. 全线上系统，可完全远程进行数据标注工作

* 设计并指导数据分发系统
  1. 标注数据自动化分发到具有相应权限的算法研发工程师，提高研发效率
  2. 自动更新标准测试数据库，有效跟进研发进度。

### __深度学习部署系统__ `2019,2020,2021 @翎腾`

* 服务端移植综合方案
  1. 设计并指导研发完整的线上部署方案(MLOps)
  2. 服务端方案有效实现完整的推理自动化，大幅提高算法工程师算法在服务端实现的效率
  3. 实现在算法工程师在PC平台只要拥有代码，就能到处移植运行。便于快速测试验证，并且拥有版本。
  4. 部署方案同时拥有算法指标验证功能，能够进行特定测试机制线上自动化运行。 

* 嵌入式端和设备端移植综合方案
  1. 针对不同的硬件设备提供商（Intel, Rockchip瑞芯微, google, 亿智）进行深度学习算法的移植工作
  2. 针对安卓系统或者IOS系统使用相应的算法框架（tflite, NCNN, MNN）进行深度学习的算法移植
  3. 常规使用的模型框架包括 tensorflow, pytorch, onnx, paddle-paddle.
  4. 完整支撑移植过程中的模型转化、参数量化、精确度保证等问题，成功进行超过30个不同的框架不同计算规模算法的嵌入式移植。


## 其他经验细节 [ 2015 - 2019.4]


### __人类运动生理相关人工智能识别检测系统__ `2020，2021 @第五区`

* 基于心率变异性的VAD情绪识别系统
* 基于多导检测系统的科学睡眠检测系统
* 基于心率和心率变异性的压力识别系统


### __人工智能运动生理指标识别检测系统__ `2018,2019 @第五区`

* 独立开发系统三部分核心算法
  1. 基于Photoplethygraph(PPG)技术的心率变异性水平计算算法，获得与传统ECG技术相同的准确度水平，并集成在嵌入式软件中。
  2. 基于LSTM的深度学习睡眠线上分析系统，有效甄别各类睡眠过程。 
  3. 基于嵌入式端的运动员日程运动量水平评估算法
* 与嵌入式软件团队配合进行嵌入式端算法部署实现。
* 与后端团队共同定义数据流和数据结构水平，并进行线上分析算法部署实现。


### __基于人工神经网络的音频识别__ `2018`
* 基于CNN的音频谱分析和RNN的音频目标识别
* 建立普适性框架用于算法部门内部其他机器学习任务使用。
* 负责整个项目的全部开发细节：
  1. 整理相关音频识别知识和经验，设计数据流
  2. 设计数据采集实验和标定工程 
  3. 编写GUI数据采集工具，并指导实验部门进行采集
  4. 数据预处理过程（数据事件自动标记，基础特征提取）
  5. 框架选择和本地配置
  6. 算法核心结构设计
  7. 制定评估准则并进行参数调整
  8. 执行线上模型部署

### __可穿戴设备完整算法的系统化设计和集成__ `2016,2017 @第五区`

* 长期作为主要算法工程师，进行可穿戴设备（手环形态）的各类算法的系统化设计。与嵌入式软件工程师紧密合作完成全部算法的集成工作
* 可穿戴设备内算法主要包括:
  * 基于多传感器合作下的多场景自动模式识别算法。
  * 运动场景数据信息化算法: 包括动态长效心率算法，高精度计步步频算法，运动强度水平监控，危险预警算法。
  * 心率变异性基本算法(基于PPG光电传感器)
  * 可穿戴设备形态的功能性算法，包括抬腕亮屏和佩戴脱落检测等。

### __基于非接触式心电的驾驶员疲劳异常预警系统研发设计__ `2015 @PSA`
* 独立设计建立实验系统用于采集相关电生理信号，同时进行多个传感器信号比对和选型。
* 对原始数据进行预处理并设计算法对疲劳异常相关的数据模式进行甄别。 

## 学术研究经历

### __EPFL, Hemodynamics and Cardiovascular Technology Laboratory(LHTC) Lab__ [洛桑，瑞士] `2014.9 - 2015.9`
_Research Assistant_<br>

* 对血管瘤CT成像结果进行3D建模并实现病理学计算视觉CNN模型。
* 负责血管瘤数据CT成像过程标注。
* 调查研究并将CNN模型引入到组内的研究过程中（组内之前一直使用传统图像处理方案）
* 模型识别准确率超过70%， train set 800. 
* Directed by Trachet Bram and Nikolaos Stergiopulos

### __EPFL, Defitech Chair in Brain-machine Interface(CNBI) Lab__ [洛桑, 瑞士] `2014.1 - 2014.9`
_Research Assistant_<br>
* 对脑机接口脑电EEG中可预见性事件异常模式通过RNN模型进行异常识别.
* Directed by Jose Del R.Millan

<br>


## 个人陈述

　　我从2013年开始从事深度学习相关研究工作,主攻计算机视觉和多模态深度学习方向。同时也拥有丰富的医疗影像学和数字信号处理相关工程产品经验。曾任职于标志雪铁龙(PSA)全球研发总部(法国巴黎), 瑞士国家微电子研发中心(CSEM)，回国后投入多个创业项目，一直致力于搭建最前沿的研究与产品工程化之间的桥梁。

　　我对人工智能领域的研究进展高度关注，拥有丰富的AI产品化经验，拥有独立研究以及带领团队研究和工程化落地的能力。任职多个创业公司的技术负责人和算法负责人，协作能力强大，学术研究视野广泛，产品思维全面，能够高效地与产品团队或者CEO协作，利用合适的前沿技术和出色的架构能力解决商业产品的问题。

　　我非常热衷于将最先进的研究成果应用于实际场景中，我个人的工作目标是将研究和产品实践相结合，帮助更多的人受益。因此，我希望能够加入一个创新型的公司或者研究团队，深入探索人工智能的领域，并将自己的专业知识和技能用于创造出更具有社会价值的产品。

## 语言

__中文:__ 母语　　|　　__英文:__ 专业熟练，工作常用语言（听说读写）　　|　　  __法文:__ 基础，仅日常阅读

<div STYLE="page-break-after: always;"></div> 
<br><br><br><br>
<h2></h2>


<div id="全景分割">
<h2>全景分割</h2>
🟢全景分割算法是一种计算机视觉技术，它可以将图像分成不同的区域，并为每个区域分配一个语义标签。全景分割算法在许多领域有广泛的应用，例如自动驾驶、医学影像分析、视频监控等。<br>
🟢在自动驾驶领域，全景分割算法是非常重要的。它可以帮助自动驾驶汽车实现道路标记识别、障碍物检测、车道检测等任务，从而实现对道路的理解和规划。此外，全景分割算法也被应用于无人机、机器人等领域。<br>
🟢另外，全景分割算法也在医学影像分析领域得到了广泛的应用。它可以帮助医生快速准确地分析病变区域，诊断疾病，并帮助制定治疗方案。<br>
🟢在视频监控领域，全景分割算法可以用于识别人、车、动物等物体，并对它们进行跟踪和识别，从而增强视频监控的效果。<br>
🟢综上所述，全景分割算法在许多领域都有广泛的应用，特别是在自动驾驶领域的应用最为广泛。<br>
</div>



<div id="Detectron2">
<h2>Detectron2</h2>
Detectron2是一个用于目标检测和分割的深度学习库，由Facebook AI Research开发，其目标是为计算机视觉研究者和开发者提供高效、灵活、易于使用的工具。Detectron2的主要功能包括支持多种常用的目标检测和分割算法，如Faster R-CNN、Mask R-CNN、RetinaNet等，并提供了许多优秀的特性，如分布式训练、GPU加速、模型微调等。
</div>

<div id="OpenMMLab">
<h2>OpenMMLab</h2>
OpenMMLab是一个面向计算机视觉的开源项目，旨在为研究者和工程师提供高效、灵活、易用的计算机视觉工具。OpenMMLab的功能非常丰富，包括了多个模块，如2D/3D检测、分割、姿态估计、人脸识别等。OpenMMLab使用PyTorch作为主要的深度学习框架，并提供了丰富的工具和API，如数据预处理、数据加载、模型定义、训练等，使得研究者和开发者能够更加高效地进行计算机视觉研究和应用开发。
</div>


<div id="stablediffusion">
<h2>stablediffusion</h2>
🟢Diffusion Model是由OpenAI提出的一种基于扩散过程的图像生成模型。该模型使用了一个逐步的过程，即从初始噪声图像开始，每个时间步骤都通过条件分布对当前图像进行扩散，从而生成一系列逐渐变得清晰的图像。Diffusion Model使用了可逆的动态生成过程，其中每个时间步骤都是可微分的，这使得该模型可以使用反向传播算法进行端到端的训练。<br>
🟢DDPM（Diffusion Probabilistic Models）是一种扩展的Diffusion Model。DDPM将每个时间步骤的条件分布建模为一个深度神经网络，使用对偶网络的方法进行训练，从而实现更高效的训练和更高质量的图像生成。此外，DDPM还使用了一种新的扩散过程，称为块扩散（block diffusion），可以在保持生成图像质量的同时减少生成时间和计算成本。<br>
</div>


<div id="zero-shot">
<h2>zero-shot</h2>
🟢Zero-shot 图像任务或小样本图像任务是指在只有少量或者没有标注数据的情况下，如何训练一个深度神经网络模型来完成图像分类、检测、分割等任务。这种任务常常是现实应用中的痛点之一，因为获取大量标注数据往往需要昂贵的人力、时间和成本。为解决这个问题，研究者们提出了一些方法和技术架构，下面简要介绍一些主要的方法：<br>
🔸迁移学习（Transfer Learning）：这种方法利用一个预训练的模型（如ImageNet）来提取图像的高级特征，然后在少量标注数据上进行微调，以适应新的任务。这种方法比较简单，但需要足够的预训练数据和与新任务相关的微调数据。<br>
🔸元学习（Meta Learning）：这种方法旨在训练一个模型，使其可以快速学习新任务。这种方法通常需要大量的小样本数据和合适的模型架构和优化算法。常见的元学习方法包括MAML、Reptile等。<br>
🔸生成模型（Generative Models）：这种方法使用生成模型来生成新的数据样本，以提高模型的泛化能力。生成模型可以利用已知类别的标注数据，来生成新的样本，从而扩充数据集。一些经典的生成模型包括VAE、GAN等。<br>
🔸模型集成（Model Ensembling）：这种方法使用多个模型，如模型融合、模型堆叠等，来增强模型的泛化性能。模型集成可以提高模型的稳定性和准确性，但需要更多的计算资源和模型设计和优化技巧。<br>
🟢总的来说，这些方法都可以在少量或没有标注数据的情况下完成图像任务，但需要考虑到任务的具体场景和数据特点，以选择合适的方法和技术架构。在实际应用中，还需要根据实际情况不断优化和调整模型，以达到最佳的性能和效果。<br>

🟢对于Zero-shot图像任务，CLIP提供了一种基于语言指导的零样本学习框架。CLIP使用文本描述作为图像类别标签，并利用其预训练模型学习到了图像和文本之间的对应关系，从而使得模型可以对未见过的图像进行分类。具体而言，给定一张未见过的图像，CLIP首先将其编码为一个特征向量，然后利用文本描述计算图像和文本之间的相似度，最终将其归为相似度最高的类别中。<br>
🟢这种基于语言指导的零样本学习框架可以广泛应用于各种应用场景中，如商业推荐、自然语言与图像的交互应用等。CLIP的出现为Zero-shot图像任务的研究提供了一种全新的思路和解决方案，大大拓展了零样本学习的应用领域。同时，CLIP的性能也取得了很好的成果，在许多经典的Zero-shot图像分类和检索任务上取得了state-of-the-art的效果。<br>
</div>




<div id="CLIP">
<h2>CLIP</h2>
🟢对比学习的跨模态预训练模型，其目的是让计算机能够更好地理解自然语言和图像之间的关系。CLIP采用Transformer网络结构，通过大规模的文本和图像数据进行无监督训练，学习到了一种通用的表示，可以用于多种下游任务，如图像分类、检索、生成等。<br>
🟢对于Zero-shot图像任务，CLIP提供了一种基于语言指导的零样本学习框架。CLIP使用文本描述作为图像类别标签，并利用其预训练模型学习到了图像和文本之间的对应关系，从而使得模型可以对未见过的图像进行分类。具体而言，给定一张未见过的图像，CLIP首先将其编码为一个特征向量，然后利用文本描述计算图像和文本之间的相似度，最终将其归为相似度最高的类别中。<br>
🟢这种基于语言指导的零样本学习框架可以广泛应用于各种应用场景中，如商业推荐、自然语言与图像的交互应用等。CLIP的出现为Zero-shot图像任务的研究提供了一种全新的思路和解决方案，大大拓展了零样本学习的应用领域。同时，CLIP的性能也取得了很好的成果，在许多经典的Zero-shot图像分类和检索任务上取得了state-of-the-art的效果。<br>
🔸...<br>
</div>


<div id="ALBEF">
<h2>ALBEF</h2>
🟢ALBEF（A Lite BERT for Adaptive Embedding of Images and Texts）是一种基于BERT的图像和文本的交互式嵌入算法，用于处理图像和文本数据的多模态融合。它由韩国电子通信研究院（ETRI）的研究人员于2020年提出。<br>
🟢ALBEF主要由两个组件组成：一个是基于Transformer的文本编码器，另一个是自适应图像编码器。这两个编码器都是基于BERT网络进行改进的。其中文本编码器主要用于将文本序列编码成一个固定长度的向量表示，而图像编码器则用于将图像转换为一个固定长度的向量表示。<br>
🟢对于文本编码器，ALBEF采用了BERT模型中的Transformer网络结构。输入文本序列先经过Embedding层转换成向量形式，然后通过多层Transformer编码器提取特征。与原始的BERT模型不同，ALBEF的Transformer编码器将使用动态卷积（Dynamic Convolution）替代标准的卷积操作，这样能够提高模型的计算效率。<br>
🟢对于图像编码器，ALBEF使用自适应的特征提取模块。具体而言，输入的图像先经过一个预处理网络，然后经过一个分级特征提取模块，该模块可以从不同的层次提取图像特征。特征提取后，ALBEF还使用了一种自适应池化方法，它可以根据输入的文本内容动态地调整特征的权重，从而使得模型更加注重与文本相关的特征。最后，经过一些全连接层的变换，图像也被转换为一个固定长度的向量表示<br>
🟢在训练时，ALBEF将最小化图像和文本之间的距离作为目标函数，同时还加入了负样本对比损失，以进一步增强模型的鲁棒性和泛化性能。<br>
🟢总的来说，ALBEF通过将文本和图像之间的交互信息嵌入到一个共同的向量空间中，能够实现图像和文本之间的语义关联，从而提高多模态数据的融合效果。它的优点在于能够有效地处理大规模数据，同时还能够实现高效的计算。<br>
</div>


<div id="DETR">
<h2>DETR</h2>
🟢DETR（DEtection TRansformer）是Facebook AI Research于2020年提出的目标检测算法，它将Transformer架构应用于目标检测领域，实现了端到端的目标检测，并在COCO数据集上取得了较好的性能。<br>
🟢DETR的主要架构由两个部分组成：编码器（Encoder）和解码器（Decoder）。编码器是一组卷积层，用于将输入图像转换为一组特征向量，这些特征向量可以被传递到解码器中。解码器包括一个Transformer编码器和一个特定于目标检测的解码器。<br>
🟢具体来说，解码器接受两个输入：编码器生成的特征向量和一组可变长度的对象查询（Object Queries）。对象查询是一组由Transformer编码器生成的特殊嵌入向量，用于表示检测框的类别和位置信息。解码器首先将这些对象查询与编码器生成的特征向量拼接在一起，然后将其输入到Transformer编码器中。Transformer编码器将生成一组新的特征向量，其中每个特征向量都对应于一个检测框，表示其在图像中的位置和类别信息。<br>
🟢然后，解码器使用一个全连接层对每个特征向量进行处理，从而得到检测框的位置和类别信息。最终，DETR使用一个损失函数来衡量模型的性能，其中包括分类损失和回归损失。<br>
🟢DETR的优点在于它不需要使用复杂的手工设计的模块，能够在不同的目标检测数据集上取得不错的性能，并且能够同时处理多个对象的检测。同时，由于采用了Transformer架构，DETR还可以利用全局信息来进行检测，从而在图像中存在大量目标的情况下取得更好的性能。<br>
</div>



<div id="BLIP">
<h2>BLIP</h2>
🟢BLIP是一个多模态混合编码器-解码器（MED）的模型架构，它由一个图像编码器（ViT），一个文本编码器（BERT），和一个文本解码器（GPT）组成1。<br>
🟢图像编码器用于提取图像特征，文本编码器用于提取文本特征，文本解码器用于生成文本。这三个模块可以根据不同的任务灵活地组合和激活12。<br>
🟢例如，对于图像-文本检索任务，可以激活图像编码器和文本编码器，并使用对比损失函数来对齐它们的特征空间；对于图像翻译任务，可以激活图像编码器和文本解码器，并使用交叉注意力机制来融合图像和文本信息12。<br>
🟢BLIP还使用了一种数据集自展（CapFilt）的方法，它可以从有噪声的图像文本对中筛选出高质量的数据进行预训练。CapFilt的核心思想是利用语言模型（LM）和视觉语言模型（VLM）来评估每个样本的质量，并根据一定的阈值来过滤掉低质量的样本2 。<br>
🔸CLIP 的数据来源于 Web 上爬来的 图像-文本对，所以数据集很容易扩充的很大，而且采用 对比学习 的方式，基本属于自监督了，不太需要做数据标注；<br>
🔸BLIP 改进了 CLIP 直接从 Web 取数据 噪声大 的缺点，提出了 Captioning and Filtering (CapFilt) 模块，这个模块就是用来 减小噪声、丰富数据 的，主要包括两个模块：<br>
🔸Captioner 字幕器：一个用于生成给定 web 图像字幕的字幕器，字幕器是一个基于图像的文本解码器，用 LM 目标函数激活，对给定图像的文本进行解码；<br>
🔸Filter 过滤器：一个用于去除噪声 image-text pair 的过滤器，过滤器是一个基于图像的文本编码器，用 ITC 和 ITM 目标函数激活，通过判断 原始文本 / 生成文本 和 图像是否匹配，用以过滤噪声文本，提高文本语料库的质量；<br>
🟢BLIP在多个视觉-语言理解和生成任务上都取得了SOTA结果，包括ImageNet-VQA、ImageNet-Captioning、MSCOCO-Retrieval、MSCOCO-Captioning等。<br>


🟢BLIP算法是一个新的视觉-语言预训练框架，可以支持比现有方法更广泛的下游任务123。根据我从网上搜索到的信息，BLIP算法可以用在以下几种下游任务：<br>

🔸图像描述（Image Captioning）：根据图像的内容生成一段描述性的文本13。<br>
🔸视觉问答（Visual Question Answering）：根据图像和用户提出的问题生成一个简短的答案13。<br>
🔸图像检索（Image Retrieval）：根据用户提供的文本或语音查询，从一个大规模的图像库中检索出最相关的图像23。<br>
🔸文本检索（Text Retrieval）：根据用户提供的图像或语音查询，从一个大规模的文本库中检索出最相关的文本23。<br>
</div>


<div id="BEiTV3">
<h2>BEiTV3</h2>
🟢BeiTV3算法是一种基于Transformer的多模态预训练模型，它可以同时处理图像和文本输入，并在不同的下游任务上进行微调。BeiTV3算法的核心思想是将图像视为一种外语，通过一个共享的词表和词嵌入层将图像和文本编码为相同的形式，然后通过多路Transformer进行交互和融合。BeiTV3算法具有高效、灵活、通用等优点，可以应对各种复杂的多模态场景。<br>

🟢BeiTV3算法的下游任务有很多，我可以列举一些例子。12<br>
🔸图像单模态任务：这些任务只涉及图像输入，比如图像分类（判断图像属于哪个类别），目标检测（找出图像中的物体并标注位置），分割（将图像分割成不同的区域）等。<br>
🔸文本单模态任务：这些任务只涉及文本输入，比如自然语言理解（理解文本的含义和逻辑），生成（根据文本生成新的文本）等。<br>
🔸图文任务：这些任务涉及同时处理图像和文本输入，比如视觉问答（回答关于图像的问题），视觉推理（判断两幅图像之间的关系）等。<br>
🔸图文检索：这种任务需要计算所有可能的图像和文本对之间的相似度，比如给定一个查询词，找出最相关的图像，或者给定一幅图像，找出最相关的描述。<br>
</div>

<div id="ITM">
<h2>ITM(image_text_maching) loss</h2>
🟢Image Text Matching Loss是一种用于图像和文本之间匹配的损失函数，通常用于训练图像和文本之间的相互对应关系。与ITC Loss类似，Image Text Matching Loss也是一种基于对比学习的损失函数，可以在图像和文本嵌入空间中学习到语义相关性。不同之处在于，Image Text Matching Loss将两个嵌入向量映射到同一维度，并通过余弦相似度来计算它们之间的距离。<br>

🟢具体来说，假设有一组图像$I={i_1, i_2, ..., i_n}$和一组文本$T={t_1, t_2, ..., t_n}$，每个图像和文本都有一个嵌入向量表示。Image Text Matching Loss的目标是通过最小化图像和文本之间的余弦距离，来学习图像和文本之间的相关性。对于每个图像$i$和对应的文本$t$，可以通过以下公式来计算它们的Image Text Matching Loss：<br>

🟢$L_{itm}(i, t) = -\log(\frac{\exp(s(i,t))}{\sum_{j=1}^{n} \exp(s(i, t_j))})$<br>

🟢其中，$s(i,t)$是图像$i$和文本$t$之间的余弦相似度，$\sum_{j=1}^{n} \exp(s(i, t_j))$是图像$i$和其他文本$t_j$之间的余弦相似度的和。因此，该损失函数的含义是：对于给定的图像和文本对$(i, t)$，它们之间的余弦相似度应该尽量大，而与其他媒介之间的余弦相似度应该尽量小。<br>

🟢通过使用Image Text Matching Loss进行训练，可以学习到一个图像和文本之间的嵌入向量空间，其中相似的媒介嵌入向量在嵌入空间中彼此靠近。这种方法可以应用于许多图像和文本之间的匹配任务，例如图像标注、视觉问答和图像文本匹配等。<br>
</div>



<div id="ITC">
<h2>ITC loss</h2>
🟢Image Text Contrastive（ITC） Loss是一种损失函数，它通常用于图像和文本之间的跨模态检索（cross-modal retrieval）任务。这个损失函数基于对比学习的思想，将图像和文本嵌入向量投射到相同的嵌入空间中，并通过最小化嵌入向量之间的欧几里得距离来学习两种媒介之间的语义关系。<br>

🟢具体来说，假设有一组图像$I={i_1, i_2, ..., i_n}$和一组文本$T={t_1, t_2, ..., t_n}$，每个图像和文本都有一个嵌入向量表示。ITC Loss的目标是让同一张图片和与之相关的文本之间的嵌入向量更加接近，而不同的图片和文本之间的嵌入向量距离则要尽量远离。因此，对于每个图像$i$和对应的文本$t$，可以通过以下公式来计算它们的ITC Loss：<br>

🟢$L_{itc}(i, t) = \sum_{j=1}^{n} \max(0, m + d(i, t) - d(i, t_j))$<br>

🟢其中，$m$是一个常数，用于指定不同的样本之间的距离阈值，$d(i,t)$是图像$i$和文本$t$之间的欧几里得距离，$d(i,t_j)$是图像$i$和其他文本$t_j$之间的欧几里得距离。因此，该损失函数的含义是：对于给定的图像和文本对$(i, t)$，它们与其他媒介$(i, t_j)$的距离应该大于一个固定的距离阈值$m$。<br>

🟢通过使用ITC Loss进行训练，可以学习到一个图像和文本之间的嵌入向量空间，其中相似的媒介嵌入向量在嵌入空间中彼此靠近。这种方法可以应用于许多图像和文本之间的跨模态检索任务，例如图像注释、视觉问答和图像文本检索等。<br>
</div>


<div id="血压">
<h2>可穿戴设备测血压</h2>
🟢最先进的血压可穿戴设备之一是Omron公司推出的HeartGuide血压手表。HeartGuide血压手表采用了传统的血压测量方法，即利用充气式袖带测量血压。它配备了一种特殊的柔性袖带，可在手表带上展开，然后通过向袖带充气来测量血压。这种测量方法比其他传感器技术更准确，但需要用户佩戴袖带并等待测量结果的时间较长。<br>
🟢HeartGuide血压手表不仅可以监测血压，还可以监测心率、步数、睡眠等生理参数。它配备了一个触摸屏，用户可以通过触摸屏来查看测量结果和其他信息。此外，HeartGuide血压手表还支持与智能手机配对，用户可以使用手机上的应用程序来跟踪和分析自己的健康数据。<br>
🟢HeartGuide血压手表是一种创新的血压可穿戴设备，它采用了传统的充气袖带测量方法，并配备了许多先进的功能，如触摸屏、配对应用程序等。它可以帮助用户更方便地监测自己的血压和其他生理参数，并及时发现健康问题。但需要注意的是，HeartGuide血压手表的价格较高，用户需要根据自己的需求和预算选择是否购买。<br>
</div>



<div id="血氧">
<h2>血氧</h2>
🟢目前市面上最先进的血氧可穿戴设备之一是Apple Watch Series 6。Apple Watch Series 6使用了一种称为脉搏氧饱和度（SpO2）传感器，它可以通过LED和光敏传感器检测血液中氧气的含量。<br>
🔸具体来说，Apple Watch Series 6在手腕上配备了一组红外LED和绿色LED，以及一个光敏传感器。当LED发出光线时，光敏传感器会测量光线的吸收量，然后计算出血液中氧气的含量。这个过程需要用户佩戴手表并保持手腕静止，通常只需要几秒钟即可完成。<br><br>
🟢Maxim Integrated是一家半导体公司，最近推出了一款名为Maxim MAX30102的可穿戴血氧传感器模块，可以与各种可穿戴设备配合使用。该模块使用了一种称为脉冲血氧测量（PPG）技术，可以通过LED和光敏传感器检测血液中氧气的含量。<br>
🔸具体来说，MAX30102模块使用红外LED和绿色LED照射皮肤，光敏传感器会检测反射回来的光线，并根据光线的吸收量计算出血液中氧气的含量。该模块还配备了一个专门的ASIC处理器，可以提供高质量的数据处理和降噪。<br>
🔸Maxim MAX30102模块可以与Arduino和Raspberry Pi等开发板配合使用，方便开发者快速搭建自己的可穿戴血氧监测设备。此外，该模块还具有低功耗、小尺寸、高精度和易于集成等优点，适用于多种应用场景。<br>
🔸需要注意的是，虽然Maxim MAX30102模块具有一定的精度和可靠性，但它仍然不是医疗设备，不能用于临床诊断或治疗。如果用户有任何健康问题，应咨询医生进行诊断和治疗。<br>
</div>




<div id="">
<h2></h2>
🟢...<br>
🔸...<br>
</div>



<div id="">
<h2></h2>
🟢...<br>
🔸...<br>
</div>

